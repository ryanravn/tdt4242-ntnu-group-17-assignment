---
task: "2.1"
title: Prompting Strategy for Code Generation
course: TDT4242
group: 17
deadline: 2026-03-02T23:59:00
score: null
status: not-submitted
---

# Task 2.1 - Prompting Strategy for Code Generation

## Description

Select one self-contained group of requirements (i.e. requirements that can be implemented independently from other groups) consisting of at least six (6) functional requirements and two (2) non-functional requirements. This group should represent a meaningful feature set of the system (for example, AI-usage logging or user authentication) and be suitable for isolated design and implementation.

**Example scope of the requirements:**

- *Functional requirement (user story):* As a student, I want to log how I used an AI tool for an assignment, so that my AI usage is documented and transparent.
- *Non-functional requirement (user story - usability):* As a student, I want the AI-usage logging interface to be simple and quick to use, so that logging does not interrupt my study workflow.

This task focuses on designing a structured prompting strategy for building components of the AIGuidebook website using AI-assisted code development tools (e.g. ChatGPT, GitHub Copilot, or similar). NTNU provides GitHub Copilot licences for students, and the InnSpill platform offers free access; additional tools may require personal licences.

Using the requirements and dependency map from Task 1.3, you will design a systematic prompting approach that supports:

- Requirement-driven code generation
- Component-level architectural planning
- Iterative refinement and debugging
- Effective human-AI co-development

The goal is not to write code, but to define a repeatable prompting strategy that enables developers to reliably generate and validate React components aligned with real system requirements. Your strategy must demonstrate understanding of a full-stack web architecture, for example React for frontend, and Node.js + Express for backend.

## Expected Outcome

Prompt Strategy Document (maximum 500 words) describing a prompting approach and patterns used.

## Evaluation Criteria

- **Quality of Prompt Strategy:** The strategy is coherent, repeatable, and covers known prompting patterns (at least 2).
- **Technical Understanding of full-stack web development:** Clear awareness of a full-stack web project structure, component organisation, and architectural concerns.

## Our Submission

### Selected Requirements

We select the **Logging & Risk Engine** group: RE-09 through RE-16 (8 functional requirements) plus NFR-01 (logging responsiveness < 2s) and NFR-02 (reply time < 30s). This group covers a self-contained data pipeline from real-time usage logging through risk classification and advisor alerting, making it suitable for isolated design, implementation, and testing.

### Prompting Approach

Our strategy uses three established prompting patterns, **Prompt Chaining**, **Iterative Refinement**, and **Few-Shot Prompting**, organized into a four-phase test-driven development pipeline.

### Phase 1: Architecture Scoping & Scaffolding

The AI receives the complete requirement set alongside domain context describing the AIGuidebook system. The prompt asks it to analyze the requirements and propose a full-stack architecture, justifying each choice against specific requirements. For example, NFR-01's 2-second constraint informs database selection, and RE-14's declaration-versus-log comparison motivates a relational model. Critically, the project is **scaffolded using standard CLI tooling** (e.g., `bunx create-hono`, `bunx create-vite`) rather than having the AI generate boilerplate from scratch. This ensures a known-good foundation with correct configurations. The AI guides which commands to run and how to configure the result. The output — a working skeleton with the chosen stack — becomes the input context for all subsequent prompts (**prompt chaining**).

### Phase 2: Test Specification (Red)

Processing **one requirement at a time**, the AI writes a single end-to-end backend test that verifies the requirement. The prompt includes the architecture from Phase 1 and the full requirement text. The test must fail because no implementation exists yet, and we confirm the failure before advancing. After the first requirement (RE-09) is fully implemented and tested, its completed test-and-implementation pair serves as a **few-shot example** in prompts for subsequent requirements, showing the AI the expected format, structure, and quality level.

### Phase 3: Minimal Implementation (Green)

For the same requirement, the AI receives the failing test and is prompted to write the minimal backend code (route handler, service logic, data model) to make it pass, and nothing beyond what the test requires. If the test still fails, error output is fed back to the AI in a feedback loop (**iterative refinement**) until the test passes. Only then do we return to Phase 2 for the next requirement.

The Phase 2 -> Phase 3 cycle repeats sequentially through each requirement (RE-09, then RE-10, through RE-16), never batching multiple requirements at once. This enforces disciplined red-green TDD.

### Phase 4: Refactor and Integration

After all requirements have passing tests, the AI reviews the codebase for refactoring opportunities and non-functional performance targets. ESLint enforces code quality throughout; AI-generated code must pass linting before advancing. All existing tests must continue to pass (**iterative refinement**).

### Architectural Awareness

This strategy produces a full-stack web application with clear separation: a frontend consuming a backend REST API backed by a persistent database. The backend-first, test-driven approach is deliberate, since backend tests execute faster than browser-based testing and enable rapid feedback during AI-assisted development. The frontend is developed last, consuming the verified API.

## Feedback

*Not yet graded.*
