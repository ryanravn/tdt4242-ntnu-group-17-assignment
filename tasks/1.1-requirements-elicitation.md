---
task: "1.1"
title: Requirements Elicitation
course: TDT4242
group: 17
deadline: 2026-01-30T23:59:00
score: 4/5
status: submitted
attempt: 1
submitted: 2026-01-30T15:39:20
---

# Task 1.1 - Requirements Elicitation

## Description

In this task, you will take on the role of a requirements engineer working in the early analysis phase of the AIGuidebook system. The goal is to understand stakeholder needs and translate them into clear, well-structured software requirements. Your main responsibility is requirements elicitation: discovering what the system should do, how it should behave, and what constraints it must respect.

You will conduct two interviews, each with a different type of stakeholder:

- **iRE Agent (Interactive Requirements Elicitation Agent):** An AI-based agent that simulates a knowledgeable stakeholder with insight into the AIGuidebook system, institutional rules, and AI-usage policies. Treat it as if you were interviewing a system owner or domain expert.
- **A real student (from your class or university):** The student should be someone who frequently uses AI tools for studying. Treat this person as an end user of the AIGuidebook system.

For each interview, you must:

- Ask open-ended questions (avoid yes/no questions)
- Use follow-up and probing questions to clarify and deepen understanding
- Explore topics such as: user goals and motivations, current workflows and practices, pain points and challenges, constraints (technical, institutional, ethical) and assumptions and expectations

> Simply "chatting" with the AI or asking shallow questions is not sufficient. You are expected to conduct interviews as a professional requirements engineer would.

Based on the two interviews, you must synthesize the elicited information into software requirements. Your requirements must have good quality, and expressed using a user story format. The requirement list should include both functional requirements (describe what the system should do) and at least two non-functional requirements (ie. Usability and data privacy).

## Expected Outcome

A requirements document containing:

1. **Interview Transcripts:** Two complete transcripts, one with the iRE Agent and one with a real student. Include both questions and answers.
2. **Functional Requirements** (as user stories) and **non-functional requirements** (as user stories or clearly labeled constraints). All requirements must follow a user story template, for example: *As a [user role], I want to [goal], so that [reason/value].*

## Evaluation Criteria

1. **Quality of Questions:** Questions are purposeful, open-ended, and relevant. Avoids superficial or generic prompting.
2. **Quality of Requirements:** Requirements are clear and unambiguous, well-structured and logically grouped, reasonably testable where applicable. Requirements reflect the overall system scope, not just isolated features.
3. **Correct Use of User Stories:** All requirements follow the user story format. User roles and goals are clearly identified.

## Our Submission

> Submitted: 2026-01-30 at 15:39:20
> Attached file: `Task1-1769783959953-958353626.pdf`

### iRE Agent Interview

*(Each new chat session separated by ---)*

**Q: Can a student just copy and paste the question in the chat and then receive direct answers?**

A: The AI Guidebook is designed to encourage responsible AI usage and academic integrity. Simply copying and pasting questions for direct answers undermines the learning process and does not align with institutional policies. The system shall provide guidance on how to engage with AI tools effectively, promoting reflection and understanding rather than mere replication of responses.

**Q: If students paste an assignment question directly into the chat, what should the system do differently compared to a normal prompt? Please describe the expected system behavior.**

A: The system shall detect when an assignment question is pasted directly into the chat and respond with a prompt encouraging the student to reflect on the question rather than simply providing an answer. It shall guide the student to break down the question into components, ask clarifying questions, and suggest resources or frameworks for approaching the problem. Additionally, the system shall log this interaction as a potential area for compliance checks, ensuring that students engage with the material meaningfully.

**Q: Are there any restrictions or safeguards the system should apply to prevent direct answer generation for assignment questions? If so, how should they work?**

A: The system shall implement safeguards to prevent direct answer generation for assignment questions by employing content filtering mechanisms that identify and flag pasted assignment queries. Upon detection, it shall respond with a structured intervention that includes prompts for reflection, guidance on breaking down the question, and encouragement to engage with the material critically. Furthermore, the system shall maintain a log of such interactions to monitor compliance with academic integrity policies and facilitate feedback for further student development.

**Q: How should the system guide students to reformulate or reflect on their question instead of pasting it directly? Can you describe a concrete interaction flow?**

A: The system shall initiate the following interaction flow upon detecting a pasted assignment question:

1. **Detection**: Upon recognizing the pasted query, the system alerts the student with a message: "It looks like you've pasted an assignment question."
2. **Encouragement**: It shall then encourage the student with a prompt: "To enhance your understanding, try reformulating the question. What are the key concepts here?"
3. **Guidance**: The system will provide structured questions to help the student reflect, such as:
   - "What is the main objective of this assignment?"
   - "Can you identify any specific terms or concepts that need clarification?"
4. **Supportive Resources**: It will suggest relevant resources or frameworks to aid in their analysis, such as links to study guides or examples.
5. **Logging**: Finally, the interaction will be logged for compliance checks, ensuring that the student's engagement is documented and monitored for academic integrity.

**Q: Should the system attempt to detect copy-pasted assignment questions, and if so, what should happen when such detection occurs?**

A: The system shall indeed attempt to detect copy-pasted assignment questions. Upon detection, it shall trigger a response that educates the student about the importance of engaging with the material critically. The system will prompt the student to analyze the question by asking guiding questions, providing resources for better understanding, and log the interaction for compliance monitoring. This approach fosters academic integrity and encourages meaningful engagement with the content.

---

**Q: What are the primary goals of the AI Guidebook system from an institutional perspective?**

A: From an institutional perspective, the primary goals of the AI Guidebook system are to ensure compliance with academic integrity policies, promote responsible use of AI tools among students, and facilitate transparency in AI-assisted academic work. The system aims to provide guidelines that align with institutional standards, support ethical AI usage, and enhance students' understanding of their responsibilities in leveraging AI technologies. Additionally, it should help institutions monitor AI usage patterns to identify trends and areas for improvement in academic practices.

**Q: You mentioned that the system should provide guidelines aligned with institutional standards. How should these guidelines be delivered to students (e.g., static pages, contextual prompts, mandatory acknowledgment)? And at what points in their workflow should they appear?**

A: Guidelines should be delivered through a combination of static pages for comprehensive information and contextual prompts that appear during relevant tasks, such as submitting assignments or logging AI usage. Mandatory acknowledgment could be implemented at key decision points, ensuring students affirm their understanding of the guidelines before proceeding. This approach reinforces awareness and promotes responsible AI usage throughout their academic workflow.

**Q: You stated that transparency in AI-assisted work is a goal. What specific information should students declare about their AI usage, and when should this declaration be required during the assignment workflow?**

A: Students should declare specific information about the AI tools used, the nature of the assistance provided, and how it contributed to their work. This declaration should be required at the point of assignment submission, ensuring that students reflect on and document their AI usage before finalizing their work. Additionally, a preliminary declaration could be prompted when they begin the assignment to encourage ongoing awareness of their AI usage throughout the process.

**Q: You mentioned monitoring AI usage patterns. What level of detail should institutions be able to see, and how should the system ensure student privacy while collecting this data?**

A: Institutions should have access to aggregated data on AI tool usage, including frequency, types of tools used, and overall patterns without identifying individual students. The system must implement privacy-aware data handling practices, such as anonymizing data and providing clear data usage policies to students. This ensures that while institutions can monitor trends for improvement, individual student privacy is safeguarded, maintaining compliance with data protection regulations.

**Q: When providing ethical AI usage, should the system only display general principles, or should it actively guide students with prompts or warnings based on their actions?**

A: The system should actively guide students with contextual prompts or warnings based on their actions, in addition to displaying general principles. This proactive approach helps students navigate ethical considerations in real-time, reinforcing responsible AI usage as they engage with the system. By providing tailored guidance, the system enhances students' understanding of ethical implications and encourages adherence to institutional standards throughout their workflow.

---

**Q: What types of AI usage patterns should the system be able to detect or flag automatically? Should the system classify AI usage risks into levels (e.g. low, medium, high), and how should those levels be defined?**

A: The system should be able to detect patterns such as frequency of AI tool usage, types of tasks being assisted by AI, and any discrepancies between self-reported AI usage and logged data. It should also flag instances of excessive reliance on AI for critical academic work, which may indicate potential academic integrity issues.

Classifying AI usage risks into levels can be beneficial. For example, low risk might involve occasional AI assistance for minor tasks, medium risk could relate to regular use for larger assignments without proper acknowledgment, and high risk may indicate heavy reliance on AI tools for core academic outputs. Each level should be defined by the extent of AI influence on the student's work and alignment with institutional guidelines.

**Q: You mentioned low, medium, and high AI usage risk levels. What concrete criteria or indicators should the system use to classify a student's AI usage into each level?**

A: For low risk, the criteria could include occasional use of AI tools for minor tasks (e.g., grammar checking, summarizing). Medium risk might be indicated by consistent use for larger assignments without proper acknowledgment or frequent assistance on complex tasks. High risk should be classified by excessive reliance on AI for fundamental academic outputs, such as major essays or projects, especially if there are discrepancies between self-reported usage and logs. Each level should incorporate the frequency, type of tasks, and adherence to institutional guidelines to ensure proper alignment with academic integrity standards.

**Q: Once a student's AI usage is classified into a risk level, what actions should the system take for each level (low, medium, high)?**

A: For low risk, the system could provide positive reinforcement, such as acknowledging responsible AI use and offering tips for further enhancement. For medium risk, it should trigger a notification encouraging the student to reflect on their AI usage, possibly suggesting resources for proper acknowledgment and responsible practices. For high risk, the system should initiate a more formal intervention, such as alerting academic advisors or prompting a mandatory review of AI usage practices, ensuring the student understands the implications for academic integrity. Each action aligns with fostering responsible AI use while supporting adherence to institutional guidelines.

**Q: At what points in the student workflow should AI usage risk classification be performed (e.g., continuously, at assignment submission, periodically)?**

A: AI usage risk classification should be performed continuously to monitor real-time usage patterns and flag potential issues as they arise. Additionally, a more formal classification should occur at assignment submission to evaluate AI reliance for specific tasks. Periodic assessments could also be scheduled, such as weekly or monthly reviews, to encourage ongoing reflection on AI usage and ensure adherence to academic integrity standards over time. This multi-faceted approach will provide a comprehensive understanding of student behavior and support responsible AI usage.

**Q: If the system detects a discrepancy between self-reported AI usage and logged data, how should the system handle this situation, and should it affect the risk level?**

A: Upon detecting a discrepancy, the system should flag the instance for review and prompt the student to clarify their reported usage. This could involve a notification or a request for an explanation. Such a discrepancy should typically elevate the risk level, as it may indicate a lack of transparency or understanding of responsible AI use. Addressing this proactively will help reinforce the importance of accurate reporting in maintaining academic integrity.

### Student Interview

**Q: Hey, thanks for talking with me. I just want to understand how students actually use AI for studying. So first, very simply how do you usually use AI when working on assignments?**

A: Honestly I use it for almost everything. Like, to get started faster, to explain things I don't understand, to help me write drafts, and sometimes just to get the answer and then rewrite it a bit.

**Q: Okay, so what's the main reason you use AI?**

A: I just want to finish the assignment as fast as possible. I have too many courses and deadlines, so AI saves me a lot of time.

**Q: What kind of AI tools do you use most?**

A: Mostly ChatGPT. Sometimes Copilot for coding. If I'm really stuck, I just paste the question and see what it gives me.

**Q: When you use AI Guidebook, what might be the most annoying or frustrating part?**

A: Probably figuring out what I'm allowed to do. Like, I never know if this is okay or if I'm crossing some line.

**Q: Can you give me a concrete example?**

A: Like if I ask ChatGPT to write a paragraph, is that cheating? What if I rewrite it? What if I only change some words? I always feel like I'm guessing.

**Q: So how do you deal with that uncertainty right now?**

A: I mostly just hope for the best. Or I try to hide it better. I don't really have a clear rule to follow.

**Q: Do you usually keep track of how you used AI for each assignment?**

A: No, not really. I just use it, finish the task, and move on.

**Q: If the teacher asked you to declare AI usage, what would be the hardest part?**

A: Remembering what exactly I did. Like, I used AI yesterday, but I don't remember what I asked it.

**Q: So when would it be easiest for you to log AI usage?**

A: Probably right when I use it. Or at least when I start an assignment and when I submit it.

**Q: If the system automatically asked you questions about AI usage, how would you want that to work?**

A: Please make it quick. Like checkboxes, or multiple choices. I don't want to write an essay about my cheating habits.

**Q: What are you most afraid of when using AI for assignments?**

A: That the teacher will think I copied everything. Even when I didn't. I'm scared of being accused of cheating by mistake.

**Q: So if a system warned you about risky AI usage, what would you want it to do?**

A: It better tell me before I submit. Like "Hey, this might be risky, you should change something." Not like a feedback after it's already too late.

**Q: Would you want the system to block submission if it thinks something is wrong?**

A: Just warn me. Let me decide.

**Q: Would you like the system to explain why something is risky?**

A: Yes, definitely. Otherwise I'd have just lose points. If it explains the rule, I'll probably fix it.

**Q: How do you feel about the teacher seeing your AI usage data?**

A: That's little scary. I'm okay with statistics, but not my name attached to it. I don't want someone judging me for using AI a lot. Just make it anonymized.

**Q: Would you ever want to see your own AI usage history?**

A: Actually yeah. It might be useful to know how much I rely on it.

**Q: If there is an actual system like this existed, what would make you not use it?**

A: If it's slow or if it asks too many questions. Or if I feel like it's just there to catch me cheating.

**Q: And what would make you like it?**

A: If it helps me with sufficient suggestions to avoid trouble and saves time.

### Requirements

**Non-functional requirements:**

1. As a user of the AI Guidebook, I want to log AI usage in under 2 seconds, so that there is a smooth user engagement.
2. As a user of the AI Guidebook, I want the software to reply to the student's message in under 30 seconds, so that it can be competitive among other products.
3. As a user of the AI Guidebook, I want the software to be available 99.9% of the time.

**Functional requirements:**

#### Guidance & Policy

- **RE-01:** As a student, I want to access a static page of institutional AI guidelines so that I can review rules at any time.
- **RE-02:** As a student, I want to see contextual guideline prompts during logging so that I am reminded of rules at relevant decision points.
- **RE-03:** As an administrator, I want to require students to acknowledge AI guidelines before they proceed to submission to ensure explicit compliance.
- **RE-04:** As a student, I want the system to explain if a specific action (e.g., paraphrasing) is allowed so that I do not accidentally violate integrity policies.

#### Intelligent Scaffolding

- **RE-05:** As an administrator, I want the system to detect when an assignment question is pasted into the chat to trigger integrity protocols.
- **RE-06:** As a student, I want the system to block direct answer generation for pasted questions to preserve academic integrity.
- **RE-07:** As a student, I want structured reflective prompts when a question is detected to help me break down the problem manually.
- **RE-08:** As a student, I want the system to suggest relevant academic resources or frameworks to support independent problem-solving.

#### Logging & Transparency

- **RE-09:** As a student, I want to log AI usage sessions in real-time using quick-input checkboxes (e.g., task types) to minimize workflow friction.
- **RE-10:** As a student, I want to link each log entry to a specific assignment or course activity for accurate record-keeping.
- **RE-11:** As a student, I want to declare tools used and the nature of AI assistance at the point of submission for full transparency.
- **RE-12:** As a student, I want to view my own usage history to reflect on my level of AI reliance over time.

#### Risk Engine

- **RE-13:** As an administrator, I want the system to classify usage into Low, Medium, and High risk based on task type and frequency.
- **RE-14:** As an administrator, I want to compare student declarations with logged data to detect reporting discrepancies automatically.
- **RE-15:** As an administrator, I want the system to perform a formal risk classification exactly at the time of assignment submission.
- **RE-16:** As an administrator, I want the system to alert academic advisors when a student is classified as "High Risk" for immediate follow-up.

#### Feedback & Privacy

- **RE-17:** As a student, I want a summary of my risk status with tips for improvement so that I can build better AI habits.
- **RE-18:** As a student, I want to complete a mandatory review of AI practices if my usage is High Risk to ensure I understand the implications.
- **RE-19:** As a student, I want my identity to be anonymized in aggregated institutional reports to protect my privacy.
- **RE-20:** As an administrator, I want to view aggregated statistics on tool usage trends to improve institutional academic practices.

## Feedback

> **Score: 4/5**

During the interviews, you did a good job gathering specifics where necessary, and asking further questions to clarify things. You could have done a better job in asking more "how" questions, but the questions you asked were still open. Watch out when asking multiple questions in a row, as with human interviews this will cause confusion, and you will not get all the answers you want/need.

The requirements show a good understanding of the task, and are formatted well. You can still improve them by extending into the lower levels of the system; Where do the users log, how is the information stored, how are the several functionalities shown to the students, etc.
